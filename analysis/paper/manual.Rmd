---
title: "PITcleanr & DABOM User Manual for Asotin Steelhead"
author:
- Kevin See:
      email: Kevin.See@dfw.wa.gov
      institute: [wdfw]
      correspondence: true
institute:
  - wdfw: Washington Department of Fish & Wildlife
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::pdf_document2:
      fig_caption: yes
      fig_height: 5
      fig_width: 6
      toc: yes
      includes:
        in_header: ../templates/header_WDFW.tex
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks2.lua
      - --lua-filter=../templates/pagebreak.lua
    wdfwTemplates::wdfw_html_format2:
      fig_caption: yes
      fig_height: 5
      fig_width: 6
      toc: yes
      toc_depth: 3
      toc_float:
        collapsed: yes
        smooth_scroll: yes
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::html_document2:
      fig_caption: yes
      fig_height: 6
      fig_width: 6
      toc: yes
      toc_depth: 3
      toc_float:
        collapsed: yes
        smooth_scroll: yes
      theme: flatly
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
    bookdown::word_document2:
      fig_caption: yes
      fig_height: 4
      fig_width: 6
      toc: yes
      reference_docx: "../templates/ReportTemplate.docx" # Insert path for the DOCX file
      pandoc_args:
      - --lua-filter=../templates/scholarly-metadata.lua
      - --lua-filter=../templates/author-info-blocks.lua
      - --lua-filter=../templates/pagebreak.lua
bibliography:
- ../paper/references.bib
references:
- id: Waterhouse2020
  title: A Bayesian nested patch occupancy model to estimate steelhead movement and abundance
  author:
  - family: Waterhouse
    given: Lynn
  - family: White
    given: Jody
  - family: See 
    given: Kevin
  - family: Murdoch
    given: Andrew
  container-title: Ecological Applications
  volume: 30
  URL: 'https://doi.org/10.1002/eap.2202'
  DOI: 10.1002/eap.2202
  issue: 8
  publisher: Ecological Society of America
  type: article-journal
  issued:
    year: 2020
    month: 12
csl: "../templates/american-fisheries-society.csl" # Insert path for the bib-style
abstract: |
  This manual contains instructions on how to run the DABOM model to estimate adult abundace for steelhead to locations in the Asotin River basin. We start by describing how to query for detections of a selected list of PIT tags using PTAGIS. We then "clean up" the detections using the R package PITcleanr. Finally we describe how to write the JAGS model for use in DABOM, and finally, run DABOM to estimate transition probabilities for steelhead throughout the Asotin system. DABOM movement probabilities can then be multiplied by estimates of escapement at the Asotin weir to get abundance to locations or tributaries.
keywords: |
  PITcleanr; DABOM; steelhead; Asotin; abundance
highlights: |
  These are the highlights.
---

```{r setup, echo = FALSE, warning=F, message=F}
# setwd('analysis/paper')
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  comment = "#>"
)

library(kableExtra)
library(here)
```

# Introduction

This manual describes how to run the **D**am **A**dult **B**ranch **O**ccupancy **M**odel ([DABOM](https://github.com/KevinSee/DABOM)) for steelhead crossing over Asotin River weir and into the Upper Columbia River. We start by describing how to query [PTAGIS](https://www.ptagis.org/) to get all detections of adults at relevant observation sites (e.g., weirs, PIT tag arrays, etc.) for a particular spawning run from a list of "valid" PIT tags. Observation data are then "cleaned up" using the `PITcleanr` R package to determine a final destination or spawning location for each individual and detection data are prepared for use in the `DABOM` R package and model. Next, we describe how to write a JAGS model for use in `DABOM`, and finally, run `DABOM` to estimate detection and movement probabilities in the Upper Columbia River system. Movement probabilities can then be multiplied by an estimate of adult escapement at Asotin River weir to estimate escapement, with uncertainty, at any observation site (or tributary) within the Upper Columbia River. 

# Set-up

## Software

The first step is to ensure that all appropriate software and R packages are installed on your computer. ([R](https://cran.r-project.org/)) is a language and environment for statistical computing and graphics and is the workhorse for running all of the code and models described here. R packages are collections of functions and data sets developed by the R community for particular tasks. Some R packages used here are available from the general R community ([Available CRAN Packages](https://cran.r-project.org/web/packages/available_packages_by_name.html)) whereas others (e.g., `PITcleanr`, `DABOM`) are developed by ([Kevin See](https://github.com/KevinSee)) and contain functions written for cleaning and analysis of PIT tag detection site and observation data.

First, you will need to have [R](https://cran.r-project.org/) downloaded and installed. Use the "base" distribution and all default installation settings should work just fine. Additionally, although not necessary, we find it very useful to use [RStudio](https://rstudio.com/) as an interface for R. Download the Desktop version of RStudio, and again, default installation settings should work just fine. RStudio provides a graphical user interface (GUI) for R with a text/code editor and allows for direct code execution, management of R packages, a viewing of R objects (e.g., data) in the environment.

Next, you will also need the [JAGS](http://mcmc-jags.sourceforge.net/) software to run DABOM. You can download that from [SourceForge](https://sourceforge.net/projects/mcmc-jags/files/). JAGS (Just Another Gibbs Sampler) software is used by `DABOM` for Bayesian inference.

## R Packages

After installing R and Studio, you will also need to install `tidyverse`, a series of R packages that work together for data science (i.e. data cleaning and manipulation), as well as the `rjags` package to interface with JAGS. To save some results to Excel files, we use the `writexl` pacakge. The `tidyverse`, `rjags` and `writexl` packages are all available from the R community and can be installed by typing the following into your R console:

```{r install-cran, eval = F}
install.packages(c("tidyverse",
                   "rjags",
                   "writexl"))
```

Next, install `PITcleanr` and `DABOM` from Kevin See's [GitHub](https://github.com/) page [here](https://github.com/KevinSee). `PITcleanr` was written primarily to build a "river network" describing the relationships among detection sites in a system, to clean PIT tag detection data to establish capture histories for individuals, and to determine the final destination or spawning location for each fish. `DABOM` is used for writing and running the `DABOM` model and estimating detection and movement probabilities. You can use `devtools` to install both of these packages from [GitHub](https://github.com/) using the following: 

```{r install-github, eval = F}
install.packages("devtools")
devtools::install_github("KevinSee/PITcleanr")
devtools::install_github("KevinSee/DABOM")
```

Hint: We have experienced errors installing the `PITcleanr` and `DABOM` packages related to *"Error: (converted from warning) package 'packagenamehere' was built under R version x.x.x"*. Setting the following environment variable typically suppresses the error and allows you to successfully install the packages.

```{r}
Sys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS = TRUE)
```

When attempting to install `PITcleanr` or `DABOM` you may receive an error message similar to *"there is no package called 'ggraph'"*. In that case, try to install the given package using the following and then attempt to install `PITclean` or `DABOM`, again.

```{r install-load-example, eval = F}
install.package("ggraph") # to install package from R cran
# replace ggraph with the appropriate package name as needed
```

We are always trying to improve the `PITcleanr` and `DABOM` R packages to minimize these types of errors.

## `devtools` Note

***IF THE devtools PACKAGE WORKS FINE ABOVE, SKIP THIS SECTION.*** To use `devtools`, you may have to download and install [Rtools](https://cran.r-project.org/bin/windows/Rtools/). You can try to use `devtools` without Rtools, initially, and if `PITcleanr` and `DABOM` fail to install correctly, installing Rtools may remedy the situation. We have had mixed results with this in the past.

# Procedure

Briefly, the steps to process the data for a given run or spawn year include:

1. Generate valid PIT tag list
1. Query PTAGIS for detections
1. Develop the "river network" describing the relationship among detection sites
1. Use PITcleanr to "clean up" detection data
1. Review PITcleanr output to determine final capture histories
1. Run DABOM to estimate detection and movement probabilities
1. Summarise DABOM results
1. Combine DABOM movement probabilities with an estimate of adult escapement at Asotin weir

Following, we describe each of the steps in detail using the 2018/2019 steelhead run as an example.

# Valid PIT Tag List and PTAGIS Query

## Tag Information

You will need to compile a list of PIT tags in steelhead that were caught in the Asotin weir for a given run or spawn year, called the valid tag list. Tagged steelhead need to be a random, representative sample of the run, and so should only include tags from the weir. If a fish is caught in the weir and happens to be previously tagged, that tag can be used as part of the valid tag list. However, if a previously tagged fish (e.g. a fish tagged as a juvenile in the Asotin) is detected upstream somewhere in the Asotin, but is not caught in the weir, it cannot be used for this analysis.

Save this list of valid PIT tag codes as a text file with no headers, to make it easy to upload to a PTAGIS query.

This is also a good opportunity to compile other relevant biological or life history information for each fish in the valid PIT tag list, such as sex, length, weight, age, origin, genetics, etc. That information may be used later to estimate, for example, sex- or age-specific abundance to locations which is useful for productivity monitoring.

```{r}
library(tidyverse)
library(lubridate) # for dealing with dates, is installed with tidyverse
library(readxl)

# which spawn year are we dealing with?
yr = 2021
```

```{r, eval = T}
# read in a list of example tags
tag_list <- read_csv(here("analysis/data/raw_data",
              "Asotin2021FemalesDABOM.csv")) %>%
  select(tag_code = `Tag Code`)

# pull out PIT tag numbers and save as a text file
write_delim(tag_list,
            file = here('analysis/data/examp_data',
                        paste0('Asotin_Sthd_Tags_', yr, '.txt')),
            delim = '\n',
            col_names = F)

```

## PTAGIS Query

The next step is to query PTAGIS for all detections of the fish included on the valid tag list. [PTAGIS](https://www.ptagis.org/) is the regional database for fish marked with PIT tags by fisheries management agencies and research organizations in the Columbia River Basin. There, go to the [Advanced Reporting](https://www.ptagis.org/data/advanced-reporting) page, which can also be found under the Data tab on the homepage. To access Advanced Reporting, you will need a free account from PTAGIS, and to be logged in. Once on the Advanced Reporting page, select "Launch" and create your own query by selecting "Create Query Builder2 Report". We will use a "Complete Tag History" query.  

You will see several query indices on the left side of the query builder, but for the purposes of `PITcleanr` and `DABOM`, we only need to deal with a couple of those. First, under "1 Select Attributes" the following fields are required to work with `PITcleanr`:

* Tag
* Mark Species
* Mark Rear Type
* Event Type
* Event Site Code
* Event Date Time
* Antenna
* Antenna Group Configuration
* Event Release Date Time
* Event Release Site Code

You are welcome to include other fields as well, but the ones listed above must be added. Any additional fields will just be included as extra columns in your query output.

The only other required index is "2 Select Metrics", but that can remain as the default, "CTH Count", which provides one record for each event recorded per tag.

Set up a filter for specific tags (e.g. the valid tag list) by next navigating to the "28 Tag Code - List or Text File" on the left. And then, after selecting "Tag" under "Attributes:", you should be able to click on "Import file...". Simply upload the .txt file you saved in the previous step containing tag codes in the valid tag list. Under "Report Message Name:" near the bottom, name the query something appropriate, such as "PTAGIS_2018_19", and select "Run Report". Once the query has successfully completed, export the output as a .csv file (e.g. "PTAGIS_2018_19.csv") using the default settings:

* Export: Whole report
* CSV file format
* Export Report Title: unchecked
* Export filter details: unchecked
* Remove extra column: Yes

<!--
Kevin - There were minor discrepancies here between your directions and my query builder, which I wonder if stems from Windows vs. Mac. Hopefully my query builder doesn't differ from most and I changed this to something inappropriate.
-->

# PITcleanr

## Processing PTAGIS Detections

The next step is to clean up all the detections listed in the PTAGIS query. Those include every detection on every antenna; we need to condense those to a single detection for each particular array of antennas, even if the fish was detected 7 times on 3 different antennas in that array. We can use the `PITcleanr` package for this. There are a few parts to this particular step. But first, load the appropriate packages into your R environment.

```{r}
library(PITcleanr)
library(tidyverse)
```

### Build Site Configuration

The first necessary step to "cleaning up" or processing the detections is to define which sites we are going to include in the DABOM model. We will select these to be specific to the Asotin. The next step is to query PTAGIS for all the metadata associated with these sites. Again, `PITcleanr` includes a function to do this, `buildConfig()`, but you will need an internet connection to run this. The `buildConfig()` function returns information about each site in PTAGIS, including the site code, the various configuration codes, the antenna IDs, when that configuration started and ended (if it has), what type of site it is (interrogation, INT, or mark/recapture/recover, MRR), the site name, the antenna group each antenna is part of, and several other pieces of information. It also assigns a 'model node' to each antenna. The model nodes essentially define which array each antenna is part of within each site. If it is a single array (or perhaps an MRR site), all of the antennas will be assigned to the same model node. If there is a double array, the antennas in the downstream array will be assigned to the "B0" array, and the upstream antennas to the "A0" array. If there is a triple array, by default the middle array is grouped with the upper array, to help simplify the DABOM model structure. Defining upstream and downstream arrays and nodes are a necessary step to estimate detection probabilities at double (or triple) arrays. This file is what will link the PTAGIS detections to the DABOM model nodes.

```{r}
org_config = buildConfig()

```

Note, the `org_config` object contains **every** INT and MRR detection site included in PTAGIS. Note that in the code above we added information about a site that was not registered in PTAGIS. You now have the opportunity to modify this configuration file however you would like, re-assigning various antennas or sites to different nodes. 
```{r}
# customize some nodes based on DABOM framework
configuration = org_config %>%
  # focus on sites in the Asotin
  filter(site_code %in% c("ACM",
                          "ASOTIC",
                          "GEORGC",
                          "ACB",
                          "AFC",
                          "CCA")) %>%
  mutate(node = if_else(site_code == "AFC",
                        if_else(str_detect(antenna_group,
                                           "Mainstem") |
                                  str_detect(antenna_group,
                                             "MAINSTEM"),
                                "AFCB0",
                                "AFCA0"),
                        node)) %>%
  # correct a couple rkm values
  mutate(rkm = if_else(site_code == 'ASOTIC',
                       '522.234.004',
                       rkm),
         rkm_total = if_else(site_code == 'ASOTIC',
                             760,
                             rkm_total)) %>%
  mutate(latitude = if_else(site_code == "ASOTIC",
                            unique(latitude[site_code == "ACM"]),
                            latitude),
         longitude = if_else(site_code == "ASOTIC",
                            unique(longitude[site_code == "ACM"]),
                            longitude)) %>%
    filter(site_code != "ACM")
```

### Parent-Child Table

The next step is to build a parent-child table that describes which nodes are upstream of which nodes. In most cases, when modeling returning adults, the parent node is the first node the adult crosses when returning upstream to spawn and the child node is the next node upstream. The exception being nodes that occur outside of the Asotin (e.g., JD1, ICH) to account for adults that are tagged at Asotin River weir, but then later detected outside of the Asotin basin. For a small, relatively simple system like the Asotin, it might be easiest to do this by hand, and add some information from the configuration file, namely the rkm.

```{r}
parent_child <- tribble(~ parent, ~ child,
                        "ASOTIC", "ACB",
                        "ASOTIC", "GEORGC",
                        "ACB", "AFC",
                        "ACB", "CCA") %>%
  left_join(configuration %>%
              select(parent = site_code, 
                     parent_rkm = rkm) %>%
              distinct()) %>%
  left_join(configuration %>%
              select(child = site_code, 
                     child_rkm = rkm) %>%
              distinct())
```

### Clean PTAGIS Data

The final step of this data cleaning process is run the PTAGIS detections through the `prepWrapper()` function in `PITcleanr` which will assign each detection to a node in the model, compress the observations so multiple detections on the same node are collapsed into a single row, add directional movement indicators based on the parent-child table, and provide suggestions for which detections should be filtered out before running DABOM. This function can also save the output into an Excel file to be examined (by setting the `save_file = T` argument). The `min_obs_date` argument will filter out observations prior to that date, while the `max_obs_date` argument will ensure that all detections after that date are not retained. If either are left blank, no such filtering is done.

```{r, eval = T}
# get raw observations from PTAGIS
# These come from running a saved query on the list of tags to be used
ptagis_file = here("analysis/data/examp_data",
                   paste0("Asotin_Sthd_", yr, ".csv"))

# recode the PTAGIS observations of double tagged fish so that the tag code matches the TagID (not TagOther)
ptagis_obs = readCTH(ptagis_file)

# any orphaned or disowned tags?
qcTagHistory(ptagis_obs, T)

# compress and process those observations with PITcleanr
prepped_ch = PITcleanr::prepWrapper(ptagis_file = ptagis_obs,
                                    configuration = configuration,
                                    parent_child = parent_child %>%
                                      addParentChildNodes(configuration = configuration),
                                    ignore_event_vs_release = T,
                                    add_tag_detects = T,
                                    save_file = T,
                                    file_name = here('analysis/data/examp_data', 
                                                     paste0('Asotin_Sthd_', yr, '_PITcleanr.xlsx')))
```

## Examine PITcleanr Output

Within this output (either in R, or in the saved Excel file), all the rows where the field `user_keep_obs` is NA or blank must be filled in with either `TRUE` or `FALSE`. The field `auto_keep_obs` provides a suggestion, but the ultimate choice is up to the user. The `path` fild shows the path the tag must have taken to get to that detection node, while the `tag detects` field shows all the sites where that tag was actually detected.

One of the assumptions in the DABOM model is that fish are making a one-way upstream migration, which ends in their spawning location. So if a fish is detected moving past the CCA array, for example, and later seen moving past the AFC site, both of those observations cannot be kept in the model. Based on the observation dates (**min_det** and **max_det**), the user will need to decide where the final spawning location was for that fish. If it was past CCA, then the rows where the **node** is AFCA0 or AFCB0 should be marked `FALSE` in the **user_keep_obs** column, and the last rows where **node** is CCAB0 or CCAB0 should be marked `TRUE`. Instead, if it appears the fish spawned past AFC, then the CCAB0 and CCAA0 rows should be marked `FALSE`. The default action taken by the **auto_keep_obs** column is to keep the latest observation, so it would default to keeping the AFC observations and dropping the CCA ones.

The next step would be for a user to filter the prepared data for all rows with `user_keep_obs == NA`, and then fill in the `user_keep_obs` column by hand for each detection node. These decisions could be guided by the `auto_keep_obs` column (`PITcleanr`'s best guess), but could also be informed by the date of detections and the user's biological knowledge of the system. Before sending the data along to DABOM, all the missing `user_keep_obs` rows should be filled out as either `TRUE` or `FALSE`. The user can then remove all the rows where `user_keep_obs == FALSE`. For our example, we'll accept all the `auto_keep_obs` recommendations.

```{r}
filter_ch = prepped_ch %>%
  mutate(user_keep_obs = if_else(is.na(user_keep_obs),
                                 auto_keep_obs,
                                 user_keep_obs)) %>%
  filter(user_keep_obs)
```


### Summarise Information for Each Tag

At this point, some summary information can be obtained for each tag in the valid tag list, including potential spawning (i.e. final) location. This is based on the furthest model node that the tag was detected at, after filtering out unwanted observations (see [Examine PITcleanr Output]). The `summarizeTagData()` function also takes biological information obtained at the weir, and the output can be used to summarize, for example, sex ratios, age or length distributions, etc. for various nodes in the network.

```{r}
tag_summ = summarizeTagData(filtered_obs = filter_ch,
                            bio_data = NULL)
```

```{r}
tag_summ %>%
  slice(1:7) %>%
  kable(booktabs = T,
        linesep = '',
        caption = 'Example of tag summaries.') %>%
  kable_styling()
```

Congratulations! You have now prepared all of your data and detections to build and run the DABOM model.

# DABOM

## DABOM Inputs

```{r}
library(DABOM)
library(rjags)
```

For `DABOM`, the user will need

* a configuration file
* a parent-child table
* a filtered detection history (based on a PTAGIS query)
* the origin of each tag

We've constructed the first three in the sections above. For the last input, we'll need to know the origin (hatchery or wild/naturally produced) of each tag. This involves constructing a tibble with two columns: `tag_code` and `origin`.  The user might be able to do this based on data taken when tagging the fish, or it could be constructed from the PTAGIS file:

```{r}
fish_origin = suppressMessages(read_csv(ptagis_file)) %>%
  select(tag_code = `Tag Code`,
         origin = `Mark Rear Type Name`) %>%
  distinct() %>%
  mutate(origin = str_sub(origin, 1, 1),
         origin = recode(origin,
                         "U" = "W"))
```

`DABOM` currently has the ability to handle up to two types of fish (e.g. hatchery and wild). Therefore, the `origin` column of `fish_origin` should only contain a maximum of two distinct entries. Note that in the example above, we have marked all the fish with "Unknown" or "U" origin as wild, to conform to this. It is fine if all the fish have the same origin (e.g. all are wild). 

Now our data is ready for DABOM.

# Write JAGS Model

The DABOM model is implemented in a Bayesian framework, using JAGS software. JAGS requires a model file (.txt format) with specific types of syntax. See the JAGS user manual for more detail about JAGS models. The `DABOM` package contains a function to write this model file, based on the relationships defined in the parent-child table, and how nodes are mapped to sites in the configuration file. 

```{r}
library(DABOM)

# file path to the default and initial model
basic_mod_file = here('analysis/data/examp_data',
                      "Asotin_DABOM.txt")

writeDABOM(file_name = basic_mod_file,
           parent_child = parent_child,
           configuration = configuration)

```

## Modify JAGS Model

Often the user will have a parent-child table based on all the detection sites they are interested in using. However, there may have been no fish detected at some of those sites that year. Rather than let the model churn away on estimating parameters that have no data to inform them, we have devised a function to "turn off" some parameters. Some of these parameters are specific to the origin of the fish, such as turning off movement parameters past a particular site only for hatchery fish, if no hatchery fish were observed there, so the `fish_origin` input is required.

This includes setting the detection probability to 0 for nodes that had no detections (or were not in place during the fishes' migration), setting the detection probability to 100% for nodes that function as single arrays with no upstream detection sites (because there is no way to estimate detection probability there), and fixing some movement probabilities to 0 if no tags were observed along that branch. Setting the detection probability to 100% may lead to a conservative estimate of escapement past that particular site (i.e. escapement was *at least* this much), but since many terminal sites are further upstream in the watershed, where most detection probabilities are likely close to 100% already, this may not be a bad assumption to make. The function `fixNoFishNodes` updates the initial JAGS file with these improvements, writes a new JAGS .txt file, and sends a message to the user about what it has done. 

```{r}
# filepath for specific JAGS model code for species and year
final_mod_file = here('analysis/data/examp_data',
                      paste0("Asotin_DABOM_", yr, ".txt"))

# writes species and year specific jags code
fixNoFishNodes(init_file = basic_mod_file,
               file_name = final_mod_file,
               filter_ch = filter_ch,
               parent_child = parent_child,
               configuration = configuration,
               fish_origin = fish_origin)

```


# Run JAGS Model

To run the JAGS model, we need to set some initial values, create inputs in a format conducive to JAGS, and set which parameters we would like to track.

## Set Initial Values

The only initial values we need to set are for where tags are in the system, based on their observed detections. Otherwise, JAGS could randomly assign them an initial value of being in one tributary, when they are actually observed in another, which will cause JAGS to throw an error and crash. 

```{r, eval = T}
# Creates a function to spit out initial values for MCMC chains
init_fnc = setInitialValues(filter_ch = filter_ch,
                            parent_child = parent_child,
                            configuration = configuration)

```

## Create JAGS Input

JAGS requires the data to be input in the form of a list. The `writeDABOM` function has also assumed a particular format and names for the data inputs. So `DABOM` has a function to transform the data into the right format expected by JAGS:

```{r, eval = T}
# Create all the input data for the JAGS model
jags_data = createJAGSinputs(filter_ch = filter_ch,
                             parent_child = parent_child,
                             configuration = configuration,
                             fish_origin = fish_origin)
```

## Set Parameters to Save

The user needs to tell JAGS which parameters to track and save posterior samples for. Otherwise, the MCMC algorithms will run, but the output will be blank. Based on the model file, `DABOM` will pull out all the detection and movement parameters to track:

```{r, eval = T}
# Tell JAGS which parameters in the model that it should save.
jags_params = setSavedParams(model_file = final_mod_file)
```

## Run MCMC

R users have developed several different packages that can be used to connect to the JAGS software and run the MCMC algorithm. Here, we will present example code that uses the `rjags` package, which was written by the author of the JAGS software. First, the user runs an adaptation or burn-in phase, which requires the path to the JAGS model file, the input data, any initial values function, the number of MCMC chains to run and the number of adaptation iterations to run. To test that the model is working, a user may set `n.chains = 1` and `n.adapt` to a small number like 5, but to actually run the model we recommend:

* `n.chains = 4`
* `n.adapt = 5000`

To make the results fully reproducible, we recommend setting the random number generator seed in R, using the `set.seed()` function. This will ensure the same MCMC results on a particular computer, although the results may still differ on different machines.

```{r, eval = T}
library(rjags)
set.seed(5)

jags = jags.model(file = final_mod_file,
                  data = jags_data,
                  inits = init_fnc,
                  n.chains = 4,
                  n.adapt = 5000)
```

If this code returns a message `Warning message: Adaptation incomplete`, this indicates further burn-in iterations should be run, which the user can do by "updating" the `jags` object using this code (and setting the `n.iter` parameter to something large enough):

```{r, eval = F}
update(jags, 
       n.iter = 1000)
```

Finally, once the adaptation or burn-in phase is complete, the user can take MCMC samples of the posteriors by using the `coda.samples` function. This is where the user tells JAGS which parameters to track. The user can also set a thinning paramater, `thin`, which will save every nth sample, instead of all of them. This is useful to avoid autocorrelations in the MCMC output. Again, we would recommend the following settings:

* `n.iter = 5000`
* `thin = 10`

Across four chains, this results in (5000 samples / 10 (every 10th sample) * 4 chains) 2000 samples of the posteriors.

```{r, eval = T}
dabom_samp = coda.samples(model = jags,
                          variable.names = jags_params,
                          n.iter = 5000,
                          thin = 10)

```

# Results

## Detection Probability Estimates

If a user is interested in the estimates of detection probability, estimated at every node in the DABOM model, the `DABOM` package provides a simple function to extract these, `summariseDetectProbs()`. The output includes the number of distinct tags observed at each node, as well as the mean, median, mode and standard deviation of the posterior distribution. It also includes the highest posterior density interval (HPDI) of whatever $\alpha$-level the user desires (using the `cred_int_prob` argument), the default value of which is 95\%. The HPDI is the narrowest interval of the posterior distribution that contains the $\alpha$-level mass, and is the Bayesian equivalent of a confidence interval. 

```{r}
# summarize detection probability estimates
detect_summ = summariseDetectProbs(dabom_mod = dabom_samp,
                                   filter_ch = filter_ch,
                                   cred_int_prob = 0.95)
```

```{r, echo = F}
detect_summ %>%
  kable(digits = 3,
        linesep = '',
        booktabs = T) %>%
  kable_styling()
```


## Estimate Escapement

To estimate escapement past each detection point, DABOM requires two things: an estimate of the movement or transition probability past that point, and an estimate of the total escapement somewhere in the stream network. Most of the time, that estimate of total escapement is provided at the tagging or release site (e.g. Tumwater Dam), and we'll assume that's what is available for this example. 

### Compile Transition Probabilities

Compiling the transition probabilities is both a matter of ensuring that the parameters from JAGS match the site codes of each detection point (so that movement past CHL is named as such), but also multiplying some transition probabilities together when appropriate. For instance, the ultimate probability that a tag moves past CHU is the probability of moving from TUM past CHL, and then from CHL past CHU. At this time, these multiplication steps are hard-coded into a few functions, one for each version of DABOM that currently exists:

* Lower Granite Dam: `compileTransProbs_GRA()`
* Priest Rapids Dam: `compileTransProbs_PRA()`
* Tumwater Dam: `compileTransProbs_TUM()`
* Prosser Dam: `compileTransProbs_PRO()`
* Asotin Weir: `compileTransProbs_ASOTIC()`

```{r}
trans_df = compileTransProbs_ASOTIC(dabom_mod = dabom_samp,
                                 parent_child = parent_child)
```

The resulting tibble contains a column describing the MCMC chain that sample is from (`chain`) the overall iteration of that sample (`iter`), the origin, either one or two (`origin`), the parameter (`param`) which is coded such that the site code refers to the probability of moving past that site, and the value of the posterior sample (`value`). Some parameters end in `_bb`, which refers to the tags that moved past a particular branching node, but did not move into one of the subsequent branches. The "bb" stands for "black box", meaning we're not sure exactly what happened to those fish. They have spawned in an area above that branching node, but below upstream detection points, they may have died in that area (due to natural causes or fishing) or they may have moved downstream undetected and were not seen at any other detection sites. 


### Total Escapement

At the Asotin weir, there may be an estimate of total abundance that can be applied. Or if the user is only interested in proportions or distribution of spawning females, just use a total escapement of `1`. 

```{r}
tot_escp = tibble(origin = c(1,2),
                    tot_escp = c(1,0))
```

Each of the transition probabilities can then be multiplied by this total escapement, resulting in posterior samples of escapement or abundance past each detection site. If there is any uncertainty in the total escapement, we recommend bootstrapping samples of the total escapement with the appropriate mean and standard error (using the same number of bootstrap samples as they have posterior samples of transition probabilities), labeling those bootstraps with an iteration number `iter` and joining them to the `trans_df` tibble by `iter` and `origin`. 

We have included some code to help summarize those posteriors below, but the user could summarize the posterior samples however they wish.

```{r}
escp_summ = trans_df %>%
    left_join(tot_escp,
              by = "origin") %>%
    mutate(escp = tot_escp * value) %>%
    group_by(location = param,
             origin) %>%
    summarise(mean = mean(escp),
              median = median(escp),
              mode = estMode(escp),
              sd = sd(escp),
              skew = moments::skewness(escp),
              kurtosis = moments::kurtosis(escp),
              lowerCI = coda::HPDinterval(coda::as.mcmc(escp))[,1],
              upperCI = coda::HPDinterval(coda::as.mcmc(escp))[,2],
              .groups = "drop") %>%
    mutate(across(c(mean, median, mode, sd, matches('CI$')),
                  ~ if_else(. < 0, 0, .))) %>%
    mutate(across(c(mean, median, mode, sd, skew, kurtosis, matches('CI$')),
                  round,
                  digits = 2)) %>%
    arrange(desc(origin), location)
```

```{r, echo = F}
escp_summ %>%
  filter(origin == 1) %>%
  kable(caption = "Proportion of tags moving past each location. Filtered to only show wild estimates.",
        linesep = '',
        booktabs = T) %>%
  kable_styling()
```

# Updating a DABOM Model

As new detection sites are installed, the current DABOM models may need to change. The user can modify the configuration file and parent-child table through R, or manually in other software like Excel and then read those files back into R. Many of the functions in the `DABOM` package will work correctly with these updated files, including `writeDABOM`, `fishNoFishNodes`, `setInitialValues`, `createJAGSinputs` and `setSavedParams`. The user can run the JAGS model, and even pull out summaries of the detection probabilities using `summariseDetectProbs`. 

***However***, to generate estimates of escapement, the user will need to modify the appropriate `compileTransProbs_XXX` function to ensure that transition probabilities at the new sites are multiplied appropriately. 

```{r, echo = F}
unlink(basic_mod_file)
unlink(final_mod_file)
```

# References
